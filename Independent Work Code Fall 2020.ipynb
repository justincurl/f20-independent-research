{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessary Packages and Functions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "# function used to create unique IDs\n",
    "def concatenate_row(row):\n",
    "        strs = []\n",
    "        for item in row:\n",
    "            strs.append(str(item))\n",
    "        return ''.join(strs)\n",
    "    \n",
    "# split train/test data intelligently\n",
    "def split_train_test(joint_data):\n",
    "    return joint_data[joint_data['Test'] == False], joint_data[joint_data['Test'] == True]\n",
    "\n",
    "# split input_output for neural network (X, y)\n",
    "def split_input_output(df):\n",
    "    y = df.pop('Left_Saved')\n",
    "    X = df\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Importing and Cleaning Full Moral Machine Dataset Responses\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_path = './SharedResponses.csv.tar.gz'\n",
    "df_columns = [\n",
    "    'Man', 'Woman', 'Pregnant', 'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless','LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive', 'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete',\n",
    "    'FemaleDoctor', 'MaleDoctor', 'Dog', 'Cat', 'LeftHand', 'Saved', 'CrossingSignal', 'Unnamed: 0', 'PedPed', 'ExtendedSessionID'\n",
    "]\n",
    "\n",
    "left_data = None\n",
    "right_data = None\n",
    "df_reader = pd.read_csv(df_path, skiprows=3, usecols=df_columns, skipinitialspace=True, chunksize=10**6)\n",
    "counter = 0\n",
    "for chunk in df_reader:\n",
    "    counter += 1\n",
    "    chunk = chunk.dropna()\n",
    "\n",
    "# only considering pedestrian vs. pedestrian accident scenarios\n",
    "    chunk = chunk[chunk['PedPed'] != 0]\n",
    "    \n",
    "# cast as smaller data type to save memory\n",
    "    chunk = chunk.astype({'Man': 'int8',\n",
    "    'Woman': 'int8',\n",
    "    'Pregnant': 'int8',\n",
    "    'Stroller': 'int8',\n",
    "    'OldMan': 'int8',\n",
    "    'OldWoman': 'int8',\n",
    "    'Boy': 'int8',\n",
    "    'Girl': 'int8',\n",
    "    'Homeless': 'int8',\n",
    "    'LargeWoman': 'int8',\n",
    "    'LargeMan': 'int8',\n",
    "    'Criminal': 'int8',\n",
    "    'MaleExecutive': 'int8',\n",
    "    'FemaleExecutive': 'int8',\n",
    "    'FemaleAthlete': 'int8',\n",
    "    'MaleAthlete': 'int8',\n",
    "    'FemaleDoctor': 'int8',\n",
    "    'MaleDoctor': 'int8',\n",
    "    'Dog': 'int8',\n",
    "    'Cat': 'int8',\n",
    "    'Saved': np.bool,\n",
    "    'LeftHand': np.bool,\n",
    "    'CrossingSignal': 'int8',\n",
    "    'Unnamed: 0': 'string',\n",
    "    })\n",
    "\n",
    "    left_chunk = chunk[chunk['LeftHand'] == 1]\n",
    "    if left_data is None:\n",
    "        left_data = left_chunk.add_prefix('Left_')\n",
    "    else:\n",
    "        left_data = pd.concat([left_data, left_chunk.add_prefix('Left_')], ignore_index=True)\n",
    "\n",
    "    right_chunk = chunk[chunk['LeftHand'] == 0]\n",
    "    if right_data is None:\n",
    "        right_data = right_chunk.add_prefix('Right_')\n",
    "    else:\n",
    "        right_data = pd.concat([right_data, right_chunk.add_prefix('Right_')], ignore_index=True)\n",
    "    print(counter)\n",
    "\n",
    "joint_data = left_data.merge(right_data, left_on=['Left_Unnamed: 0'], right_on=['Right_Unnamed: 0'], how='inner').drop(columns=['Left_Unnamed: 0', 'Right_Unnamed: 0', 'Right_ExtendedSessionID','Left_LeftHand', 'Right_LeftHand', 'Left_PedPed', 'Right_PedPed', 'Right_Saved', 'Right_CrossingSignal'])\n",
    "\n",
    "del left_data\n",
    "del right_data\n",
    "del left_chunk\n",
    "del right_chunk\n",
    "joint_cols = [\n",
    "'Left_ExtendedSessionID',\n",
    "'Left_Man',\n",
    "'Left_Woman',\n",
    "'Left_CrossingSignal',\n",
    "'Left_Pregnant',\n",
    "'Left_Stroller',\n",
    "'Left_OldMan',\n",
    "'Left_OldWoman',\n",
    "'Left_Boy',\n",
    "'Left_Girl',\n",
    "'Left_Homeless',\n",
    "'Left_LargeWoman',\n",
    "'Left_LargeMan',\n",
    "'Left_Criminal',\n",
    "'Left_MaleExecutive',\n",
    "'Left_FemaleExecutive',\n",
    "'Left_FemaleAthlete',\n",
    "'Left_MaleAthlete',\n",
    "'Left_FemaleDoctor',\n",
    "'Left_MaleDoctor',\n",
    "'Left_Dog',\n",
    "'Left_Cat',\n",
    "'Right_Man',\n",
    "'Right_Woman',\n",
    "'Right_Pregnant',\n",
    "'Right_Stroller',\n",
    "'Right_OldMan',\n",
    "'Right_OldWoman',\n",
    "'Right_Boy',\n",
    "'Right_Girl',\n",
    "'Right_Homeless',\n",
    "'Right_LargeWoman',\n",
    "'Right_LargeMan',\n",
    "'Right_Criminal',\n",
    "'Right_MaleExecutive',\n",
    "'Right_FemaleExecutive',\n",
    "'Right_FemaleAthlete',\n",
    "'Right_MaleAthlete',\n",
    "'Right_FemaleDoctor',\n",
    "'Right_MaleDoctor',\n",
    "'Right_Dog',\n",
    "'Right_Cat',\n",
    "'Left_Saved'\n",
    "]\n",
    "joint_data = joint_data[joint_cols]\n",
    "print(joint_data.info(memory_usage='deep'))\n",
    "\n",
    "# Insert Bias Column\n",
    "all_ones = [1 for i in range(len(joint_data))]\n",
    "joint_data.insert(2, \"Bias\", all_ones)\n",
    "joint_data = joint_data.astype({'Bias': 'int8'})\n",
    "\n",
    "# Version 1 dataframe (raw saved)\n",
    "joint_data.to_hdf(\"hdfMoralMachineDataNN1\", key='dataframe')\n",
    "print('version 1 data saved')\n",
    "del joint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build and Save Version 2 of Dataset\n",
    "\n",
    "# Create IDs for each Scenario Type Category\n",
    "id_creator = pd.read_hdf(\"./hdfMoralMachineDataNN1\")\n",
    "id_creator.pop('Left_Saved')\n",
    "id_creator.pop('Left_ExtendedSessionID')\n",
    "id_creator = id_creator.apply(concatenate_row, axis=1)\n",
    "\n",
    "# Version 2 dataframe (v1 + scenario type IDs)\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNN1\")\n",
    "joint_data.insert(0, 'ID', id_creator)\n",
    "joint_data.to_hdf(\"hdfMoralMachineDataNN2\", key='dataframe')\n",
    "print('version 2 data saved')\n",
    "\n",
    "del joint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONLY USE if training neural network with naive splitting approach\n",
    "\n",
    "# Preparing test_train data with naive approach\n",
    "RANDOM_SPLIT = True\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNN1\")\n",
    "if 'Left_Saved' in joint_data:\n",
    "    joint_data.pop('Left_ExtendedSessionID')\n",
    "    y = joint_data.pop('Left_Saved')\n",
    "    X = joint_data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "del X, y, joint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset Status Check\n",
    "\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNN1\")\n",
    "joint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intelligently Prepare Data for Splitting by Grouping and Sorting by Scenario Type Categories\n",
    "\n",
    "# Read in data from version 1 and group by scenario type category (not including the respondent's answer)\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNN1\")\n",
    "joint_data.pop('Left_ExtendedSessionID')\n",
    "grouped = joint_data.groupby(joint_data.columns.tolist()[:-1]).size().reset_index()\n",
    "del joint_data\n",
    "sorted_g = grouped.sort_values([0], ascending=False)\n",
    "del grouped\n",
    "sorted_g = sorted_g.rename(columns={0: \"ScenarioType_Count\"})\n",
    "\n",
    "# Randomize split for which scenario type category is in train and which is in test      \n",
    "train_test = [False for i in range(len(sorted_g))]\n",
    "considered = 0\n",
    "while considered < len(train_test) - 5:   \n",
    "    row = int(5 * random.random())\n",
    "    train_test[row + considered] = True\n",
    "    considered += 5\n",
    "\n",
    "sorted_g.insert(42, 'Test', train_test)\n",
    "del train_test\n",
    "sorted_g\n",
    "\n",
    "# Create the IDs used for each unique scenario type category by combining all 41 col values that make up a scenario\n",
    "ids = sorted_g[sorted_g.columns.tolist()[:-2]].apply(concatenate_row, axis=1)\n",
    "\n",
    "# Map from IDs to Train/Test set based on the random assignment from the grouped data above\n",
    "mapping = {}\n",
    "for item in ids.items():\n",
    "    mapping[item[1]] = sorted_g['Test'][item[0]]\n",
    "\n",
    "del sorted_g\n",
    "del ids\n",
    "\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNN2\")\n",
    "joint_data.pop('Left_ExtendedSessionID')\n",
    "\n",
    "train_test_in_order = []\n",
    "for item in joint_data['ID']:\n",
    "    train_test_in_order.append(mapping[item])\n",
    "\n",
    "joint_data.insert(42, \"Test\", train_test_in_order)\n",
    "\n",
    "joint_data['Left_CrossingSignal'] = joint_data['Left_CrossingSignal'].replace(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Version 3 (Full Dataset prepared for intelligent split)\n",
    "\n",
    "joint_data.to_hdf(\"hdfMoralMachineDataNN3\", key='dataframe')\n",
    "print('version 3 saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Intelligently Split Prepared Dataset for Neural Network\n",
    "\n",
    "# Read in data with true/false labels (prepared for intelligent split)\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNN3\")\n",
    "\n",
    "all_ones = [1 for i in range(len(joint_data))]\n",
    "joint_data.insert(2, \"Bias2\", all_ones)\n",
    "joint_data = joint_data.astype({'Bias2': 'int8'})\n",
    "\n",
    "#  split into train/test data\n",
    "train, test = split_train_test(joint_data)\n",
    "\n",
    "#  split into X, y (input, output) data for neural network\n",
    "X_train, y_train = split_input_output(train)\n",
    "X_test, y_test = split_input_output(test)\n",
    "\n",
    "#  remove columns used for data preparation (not to be used for neural network training/testing)\n",
    "X_train.pop('ID')\n",
    "X_train.pop('Test')\n",
    "X_test.pop('ID')\n",
    "X_test.pop('Test')\n",
    "\n",
    "del train\n",
    "del test\n",
    "del joint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Build, Train, and Save Neural Network for Full Moral Machine Dataset\n",
    "\n",
    "#  Build neural network\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(32, activation='relu', input_shape=(42,)))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.Dense(2, activation='sigmoid'))\n",
    "network.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "#  Fit neural network\n",
    "history = network.fit(X_train, utils.to_categorical(y_train), \n",
    "                      epochs=100, batch_size=1024, validation_data=(X_test, utils.to_categorical(y_test)), callbacks=[es])\n",
    "\n",
    "#  Save neural network\n",
    "if RANDOM_SPLIT:\n",
    "    model_json = network.to_json()\n",
    "    with open(\"random_split_model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    network.save_weights(\"random_split_model.h5\")\n",
    "    print(\"Saved random split model to disk\")\n",
    "else:\n",
    "    model_json = network.to_json()\n",
    "    with open(\"intelligent_split_model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    network.save_weights(\"intelligent_split_model.h5\")\n",
    "    print(\"Saved intelligent split model to disk\")\n",
    "\n",
    "del X_train, y_train, X_test, y_test, model_json, history, es, network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Load and Test Standard Neural Network\n",
    "\n",
    "json_file = open('intelligent_split_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"intelligent_split_model.h5\")\n",
    "print(\"Loaded intelligent split model from disk\")\n",
    "    \n",
    "loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, utils.to_categorical(y), verbose=0)\n",
    "\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing and Cleaning Survey Data for Available Respondents\n",
    "\n",
    "data_types = {\n",
    "    'SharedResponsesSurvey.csv': str,\n",
    "    'ExtendedSessionID': str,\n",
    "    'UserID': 'float64', \n",
    "    'LeftHand': 'float16', \n",
    "    'UserCountry3':str, \n",
    "    'Review_age':'float16', \n",
    "    'Review_education': str, \n",
    "    'Review_gender': str, \n",
    "    'Review_income': str, \n",
    "    'Review_political': 'float16',\n",
    "    'Review_religious': 'float16'\n",
    "}\n",
    "\n",
    "columns = ['SharedResponsesSurvey.csv', 'ExtendedSessionID', 'UserID', 'LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious']\n",
    "survey_path = \"./SharedResponsesSurvey.csv.tar.gz\"\n",
    "\n",
    "survey = pd.read_csv(survey_path, usecols=columns, dtype=data_types)\n",
    "survey = survey.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "survey = survey.drop_duplicates(subset='UserID').reset_index(drop=True)\n",
    "\n",
    "category_columns = ['Review_gender', 'Review_education', 'Review_income', 'UserCountry3']\n",
    "for col in category_columns:\n",
    "    survey[col] = survey[col].astype('category')\n",
    "    \n",
    "int_columns = ['Review_age', 'LeftHand']\n",
    "for col in int_columns:\n",
    "    survey[col] = survey[col].astype('int8')\n",
    "\n",
    "survey['UserID'] = survey['UserID'].astype('int64')\n",
    "print(survey.info(memory_usage='deep'))\n",
    "\n",
    "survey.to_hdf(\"hdfMoralMachineDataNNSurvey\", key='dataframe', format='table')\n",
    "print('version survey data saved')\n",
    "\n",
    "del survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine Respondent data with Survey data\n",
    "\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNN1\")\n",
    "survey = pd.read_hdf(\"./hdfMoralMachineDataNNSurvey\")\n",
    "\n",
    "survey_and_full = joint_data.merge(survey, left_on=['Left_ExtendedSessionID'], right_on=['ExtendedSessionID'], how='inner').drop(columns=['Left_ExtendedSessionID', 'SharedResponsesSurvey.csv'])\n",
    "del survey\n",
    "del joint_data\n",
    "\n",
    "reorder_cols = [\n",
    "'UserID',\n",
    "'Bias',\n",
    "'LeftHand',\n",
    "'UserCountry3',\n",
    "'Review_age',\n",
    "'Review_education',\n",
    "'Review_gender',\n",
    "'Review_income',\n",
    "'Review_political',\n",
    "'Review_religious',\n",
    "'Left_Man',\n",
    "'Left_Woman',\n",
    "'Left_CrossingSignal',\n",
    "'Left_Pregnant',\n",
    "'Left_Stroller',\n",
    "'Left_OldMan',\n",
    "'Left_OldWoman',\n",
    "'Left_Boy',\n",
    "'Left_Girl',\n",
    "'Left_Homeless',\n",
    "'Left_LargeWoman',\n",
    "'Left_LargeMan',\n",
    "'Left_Criminal',\n",
    "'Left_MaleExecutive',\n",
    "'Left_FemaleExecutive',\n",
    "'Left_FemaleAthlete',\n",
    "'Left_MaleAthlete',\n",
    "'Left_FemaleDoctor',\n",
    "'Left_MaleDoctor',\n",
    "'Left_Dog',\n",
    "'Left_Cat',\n",
    "'Right_Man',\n",
    "'Right_Woman',\n",
    "'Right_Pregnant',\n",
    "'Right_Stroller',\n",
    "'Right_OldMan',\n",
    "'Right_OldWoman',\n",
    "'Right_Boy',\n",
    "'Right_Girl',\n",
    "'Right_Homeless',\n",
    "'Right_LargeWoman',\n",
    "'Right_LargeMan',\n",
    "'Right_Criminal',\n",
    "'Right_MaleExecutive',\n",
    "'Right_FemaleExecutive',\n",
    "'Right_FemaleAthlete',\n",
    "'Right_MaleAthlete',\n",
    "'Right_FemaleDoctor',\n",
    "'Right_MaleDoctor',\n",
    "'Right_Dog',\n",
    "'Right_Cat',\n",
    "'Left_Saved'\n",
    "]\n",
    "survey_and_full = survey_and_full[reorder_cols]                                                                                           \n",
    "\n",
    "survey_and_full.to_hdf(\"hdfMoralMachineDataNNCombined\", key='dataframe', format='table')\n",
    "print('survey and full combined saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Split Combined Accident Scenario Responses and Demographic Information Dataset into Training and Testing Sets\n",
    "\n",
    "# Preparing combined survey respondent Data for Neural Network\n",
    "survey_and_full = pd.read_hdf(\"./hdfMoralMachineDataNNCombined\")\n",
    "\n",
    "# Substitute strings for ints\n",
    "lst = survey_and_full['UserCountry3'].unique().tolist()\n",
    "country_map = {lst[i]: i for i in range(len(lst))}\n",
    "lst = []\n",
    "for item in survey_and_full['UserCountry3']:\n",
    "    lst.append(country_map[item])\n",
    "survey_and_full['UserCountry3'] = lst\n",
    "\n",
    "lst = survey_and_full['Review_education'].unique().tolist()\n",
    "education_map = {lst[i]: i for i in range(len(lst))}\n",
    "lst = []\n",
    "for item in survey_and_full['Review_education']:\n",
    "    lst.append(education_map[item])\n",
    "survey_and_full['Review_education'] = lst\n",
    "\n",
    "\n",
    "lst = survey_and_full['Review_gender'].unique().tolist()\n",
    "gender_map = {lst[i]: i for i in range(len(lst))}\n",
    "lst = []\n",
    "for item in survey_and_full['Review_gender']:\n",
    "    lst.append(gender_map[item])\n",
    "survey_and_full['Review_gender'] = lst\n",
    "\n",
    "lst = survey_and_full['Review_income'].unique().tolist()\n",
    "income_map = {lst[i]: i for i in range(len(lst))}\n",
    "lst = []\n",
    "for item in survey_and_full['Review_income']:\n",
    "    lst.append(income_map[item])\n",
    "survey_and_full['Review_income'] = lst\n",
    "\n",
    "# Selectively include certain demographic information to see impact on accuracy\n",
    "LEFTHAND = False\n",
    "COUNTRY = False\n",
    "AGE = False\n",
    "EDU = False\n",
    "GENDER = False\n",
    "INCOME = False\n",
    "POLITICAL = False\n",
    "RELIGIOUS = False\n",
    "ONES = False\n",
    "DOUBLE_ONES = False\n",
    "STANDARD = False\n",
    "\n",
    "if LEFTHAND:\n",
    "    survey_and_full = survey_and_full.drop(columns=['UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "elif COUNTRY:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "elif AGE:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "elif EDU:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "elif GENDER:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "elif INCOME:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_political', 'Review_religious'])\n",
    "elif POLITICAL:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_religious'])\n",
    "elif RELIGIOUS:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political'])\n",
    "elif ONES:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "    all_ones = [1 for i in range(len(survey_and_full))]\n",
    "    survey_and_full.insert(2, \"Ones\", all_ones)\n",
    "    survey_and_full = survey_and_full.astype({'Ones': 'int8'})\n",
    "elif DOUBLE_ONES:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "    all_ones = [1 for i in range(len(survey_and_full))]\n",
    "    survey_and_full.insert(2, \"Ones\", all_ones)\n",
    "    survey_and_full.insert(2, \"Ones2\", all_ones)\n",
    "    survey_and_full = survey_and_full.astype({'Ones': 'int8'})\n",
    "    survey_and_full = survey_and_full.astype({'Ones2': 'int8'})\n",
    "elif STANDARD:\n",
    "    survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Status Check on Combined Response + Demographic Dataset\n",
    "\n",
    "survey_and_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split Data with Naive Approach to Prepare for Neural Network\n",
    "\n",
    "if 'Left_Saved' in survey_and_full:\n",
    "    survey_and_full.pop('UserID')\n",
    "    y = survey_and_full.pop('Left_Saved')\n",
    "    X = survey_and_full\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "\n",
    "del X\n",
    "del y\n",
    "del survey_and_full\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build, Train, and Save Neural Network on Expanded Dataset\n",
    "\n",
    "if LEFTHAND or COUNTRY or AGE or EDU or GENDER or INCOME or POLITICAL or RELIGIOUS or ONES:\n",
    "    num_cols = 43\n",
    "else:\n",
    "    num_cols = 50\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(32, activation='relu', input_shape=(num_cols,)))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.Dense(2, activation='sigmoid'))\n",
    "network.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "#  Fit neural network\n",
    "history = network.fit(X_train, utils.to_categorical(y_train), epochs=100, batch_size=512, validation_data=(X_test, utils.to_categorical(y_test)), callbacks=[es])\n",
    "\n",
    "if LEFTHAND:\n",
    "    name = 'left'\n",
    "elif COUNTRY:\n",
    "    name = 'country'\n",
    "elif AGE:\n",
    "    name = 'age'\n",
    "elif EDU:\n",
    "    name = 'edu'\n",
    "elif GENDER:\n",
    "    name = 'gender'\n",
    "elif INCOME:\n",
    "    name = 'income'\n",
    "elif POLITICAL:\n",
    "    name = 'political'\n",
    "elif RELIGIOUS:\n",
    "    name = 'religious'\n",
    "elif ONES:\n",
    "    name = 'ones'\n",
    "elif DOUBLE_ONES:\n",
    "    name = 'double_ones'\n",
    "elif STANDARD:\n",
    "    name = 'standard'\n",
    "else:\n",
    "    name = 'all'\n",
    "\n",
    "#  Save combined neural network\n",
    "model_json = network.to_json()\n",
    "json_filename = \"survey_model_\" + name + \".json\"\n",
    "\n",
    "with open(json_filename, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "h5_filename = \"survey_model_\" + name + \".h5\"\n",
    "network.save_weights(h5_filename)\n",
    "print(\"Saved survey\", name, \"intelligent split model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load and Test Combined Neural Networks\n",
    "names = ['political', 'religious', 'ones', 'all', 'left', 'country', 'age', 'edu', 'gender', 'income']\n",
    "for name in names:\n",
    "    json_filename = \"survey_model_\" + name + \".json\"\n",
    "    json_file = open(json_filename, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    h5_filename = \"survey_model_\" + name + \".h5\"\n",
    "    loaded_model.load_weights(h5_filename)\n",
    "    print(\"Loaded\", name, \"survey model from disk\")\n",
    "    \n",
    "    # split into train and test sets for neural network\n",
    "\n",
    "    # Preparing combined survey respondent Data for Neural Network\n",
    "    survey_and_full = pd.read_hdf(\"./hdfMoralMachineDataNNCombined\")\n",
    "\n",
    "    # Substitute strings for ints\n",
    "    lst = survey_and_full['UserCountry3'].unique().tolist()\n",
    "    country_map = {lst[i]: i for i in range(len(lst))}\n",
    "    lst = []\n",
    "    for item in survey_and_full['UserCountry3']:\n",
    "        lst.append(country_map[item])\n",
    "    survey_and_full['UserCountry3'] = lst\n",
    "\n",
    "    lst = survey_and_full['Review_education'].unique().tolist()\n",
    "    education_map = {lst[i]: i for i in range(len(lst))}\n",
    "    lst = []\n",
    "    for item in survey_and_full['Review_education']:\n",
    "        lst.append(education_map[item])\n",
    "    survey_and_full['Review_education'] = lst\n",
    "\n",
    "    lst = survey_and_full['Review_gender'].unique().tolist()\n",
    "    gender_map = {lst[i]: i for i in range(len(lst))}\n",
    "    lst = []\n",
    "    for item in survey_and_full['Review_gender']:\n",
    "        lst.append(gender_map[item])\n",
    "    survey_and_full['Review_gender'] = lst\n",
    "\n",
    "    lst = survey_and_full['Review_income'].unique().tolist()\n",
    "    income_map = {lst[i]: i for i in range(len(lst))}\n",
    "    lst = []\n",
    "    for item in survey_and_full['Review_income']:\n",
    "        lst.append(income_map[item])\n",
    "    survey_and_full['Review_income'] = lst\n",
    "\n",
    "    if name == 'left':\n",
    "        survey_and_full = survey_and_full.drop(columns=['UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "    elif name == 'country':\n",
    "        survey_and_full = survey_and_full.drop(columns=['LeftHand', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "    elif name == 'age':\n",
    "        survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "    elif name == 'edu':\n",
    "        survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "    elif name == 'gender':\n",
    "        survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "    elif name == 'income':\n",
    "        survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_political', 'Review_religious'])\n",
    "    elif name == 'political':\n",
    "        survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_religious'])\n",
    "    elif name == 'religious':\n",
    "        survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political'])\n",
    "    elif name == 'ones':\n",
    "        survey_and_full = survey_and_full.drop(columns=['LeftHand', 'UserCountry3', 'Review_age', 'Review_education', 'Review_gender', 'Review_income', 'Review_political', 'Review_religious'])\n",
    "        all_ones = [1 for i in range(len(survey_and_full))]\n",
    "        survey_and_full.insert(2, \"Ones\", all_ones)\n",
    "        survey_and_full = survey_and_full.astype({'Ones': 'int8'})\n",
    "\n",
    "    if 'Left_Saved' in survey_and_full:\n",
    "        survey_and_full.pop('UserID')\n",
    "        y = survey_and_full.pop('Left_Saved')\n",
    "        X = survey_and_full\n",
    "        del survey_and_full\n",
    "\n",
    "    loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    score = loaded_model.evaluate(X, utils.to_categorical(y), verbose=0)\n",
    "    \n",
    "    \n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build and Test Neural Network on Respondents without Survey Data\n",
    "\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNN1\")\n",
    "survey = pd.read_hdf(\"./hdfMoralMachineDataNNSurvey\")\n",
    "\n",
    "no_survey = joint_data[~joint_data['Left_ExtendedSessionID'].isin(survey['ExtendedSessionID'])].drop(columns=['Left_ExtendedSessionID'])\n",
    "del survey\n",
    "del joint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset into Input and Output\n",
    "if 'Left_Saved' in no_survey:\n",
    "    y = no_survey.pop('Left_Saved')\n",
    "    X = no_survey\n",
    "    \n",
    "# Split Dataset into Test and Train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "\n",
    "del X\n",
    "del y\n",
    "del no_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(32, activation='relu', input_shape=(42,)))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.Dense(32, activation='relu'))\n",
    "network.add(layers.Dense(2, activation='sigmoid'))\n",
    "network.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "#  Fit neural network\n",
    "history = network.fit(X_train, utils.to_categorical(y_train), epochs=100, batch_size=512, validation_data=(X_test, utils.to_categorical(y_test)), callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MAML dataset for Mayank (NOT RELEVANT FOR FALL 2020 WRITTEN REPORT)\n",
    "\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNN1\")\n",
    "\n",
    "mapping = joint_data.groupby('Left_ExtendedSessionID').size().reset_index()\n",
    "\n",
    "mapping = dict(zip(mapping['Left_ExtendedSessionID'], mapping[0]))\n",
    "\n",
    "count = []\n",
    "for item in joint_data['Left_ExtendedSessionID']:\n",
    "    count.append(mapping[item])\n",
    "\n",
    "del mapping\n",
    "\n",
    "joint_data.insert(44, 'Count', count)\n",
    "\n",
    "del count\n",
    "\n",
    "joint_data = joint_data[joint_data['Count'] >= 6]\n",
    "\n",
    "joint_data[joint_data['Count'] == 6].to_hdf(\"hdfMoralMachineDataNN6\", key='dataframe')\n",
    "\n",
    "joint_data = joint_data[joint_data['Count'] > 6]\n",
    "\n",
    "joint_data = joint_data.sort_values(by=['Left_ExtendedSessionID'])\n",
    "\n",
    "joint_data = joint_data.reset_index().drop(columns=['index'])\n",
    "\n",
    "start = 0\n",
    "end = 0\n",
    "all_indices = []\n",
    "while end < len(joint_data):\n",
    "    if end % 100000 < 9:\n",
    "        print(end/len(joint_data)*100,'%')\n",
    "    count = joint_data['Count'][start]\n",
    "    end += count\n",
    "    all_indices.extend(list(joint_data[start:end].sample(n=6).index.values))\n",
    "    start = end\n",
    "    \n",
    "joint_data = joint_data.iloc[all_indices,:]\n",
    "joint_data.drop(columns=['Count'])\n",
    "\n",
    "joint_data6 = pd.read_hdf(\"./hdfMoralMachineDataNN6\")\n",
    "joint_data = joint_data.append(joint_data6)\n",
    "\n",
    "joint_data = joint_data.sort_values(by=['Left_ExtendedSessionID']).drop(columns=['Count'])\n",
    "\n",
    "joint_data = joint_data.reset_index().drop(columns=['index'])\n",
    "\n",
    "joint_data.to_hdf(\"hdfMoralMachineDataNNMAML\", key='dataframe')\n",
    "\n",
    "del joint_data\n",
    "\n",
    "joint_data = pd.read_hdf(\"./hdfMoralMachineDataNNMAML\").to_numpy()\n",
    "\n",
    "joint_data = joint_data.reshape(936982, 6, 44)\n",
    "\n",
    "np.save('jcurlmamldata.npy', joint_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
